{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1842d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c020e",
   "metadata": {},
   "source": [
    "## Loading the Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d80744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "C:\\Users\\Himanshu Singh\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\image_processing_base.py:417: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  image_processor = cls(**image_processor_dict)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"matei-dorian/segformer-b5-finetuned-human-parsing\");\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"matei-dorian/segformer-b5-finetuned-human-parsing\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f031b",
   "metadata": {},
   "source": [
    "### Loading the Example Image for Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348152cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"D:\\Code Playground\\wardrob-aI\\data\\my_photos\\person_02.png\"\n",
    "image = Image.open(img_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af0a48",
   "metadata": {},
   "source": [
    "#### Doing all the Pre-processing stuffs and sending the input to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d3555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\") # All pre-processing stuffs to the example image\n",
    "outputs = model(**inputs) # Sending input image to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5fdc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed2e8c",
   "metadata": {},
   "source": [
    "### Resizes the output image back to the original size\n",
    "\n",
    "##### Step-by-step:\n",
    "- Takes the low-resolution segmentation logits\n",
    "- Upscales them (resizes) back to full image resolution\n",
    "- Uses bilinear interpolation for smooth resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3320c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_logits = torch.nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],  # (height, width)\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False\n",
    ")\n",
    "pred_mask = upsampled_logits.argmax(dim=1)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0612a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 18\n",
      "Classes present in image: [ 0  2  4  6  9 10 11 12 14 15]\n"
     ]
    }
   ],
   "source": [
    "num_classes = logits.shape[1]\n",
    "print(\"Total classes:\", num_classes)\n",
    "unique_ids = np.unique(pred_mask)\n",
    "print(\"Classes present in image:\", unique_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3052533",
   "metadata": {},
   "source": [
    "### Defining stable color palette for ATR 18 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Predefined stable color palette for ATR 18 classes\n",
    "ATR_COLORS = {\n",
    "    0:  (0, 0, 0),          # Background - Black\n",
    "    1:  (128, 0, 0),        # Hat\n",
    "    2:  (255, 0, 0),        # Hair\n",
    "    3:  (255, 255, 0),      # Sunglasses\n",
    "    4:  (0, 128, 0),        # Upper-clothes\n",
    "    5:  (0, 255, 0),        # Skirt\n",
    "    6:  (0, 0, 128),        # Pants\n",
    "    7:  (0, 0, 255),        # Dress\n",
    "    8:  (128, 128, 0),      # Belt\n",
    "    9:  (128, 0, 128),      # Left-shoe\n",
    "    10: (255, 0, 255),      # Right-shoe\n",
    "    11: (255, 200, 150),    # Face (skin tone)\n",
    "    12: (150, 150, 255),    # Left-leg\n",
    "    13: (180, 180, 255),    # Right-leg\n",
    "    14: (255, 180, 180),    # Left-arm\n",
    "    15: (255, 150, 150),    # Right-arm\n",
    "    16: (0, 150, 150),      # Bag\n",
    "    17: (0, 255, 255),      # Scarf\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_mask(mask, save_path=\"D:/Code Playground/wardrob-ai/experiments/output/masks/parsed_output.png\"):\n",
    "    \"\"\"\n",
    "    Convert class mask → color mask using predefined ATR color palette\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    H, W = mask.shape\n",
    "    color_mask = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "\n",
    "    for class_id, color in ATR_COLORS.items():\n",
    "        color_mask[mask == class_id] = color\n",
    "\n",
    "    Image.fromarray(color_mask).save(save_path)\n",
    "    print(f\"Output mask saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebbdb2",
   "metadata": {},
   "source": [
    "### All the pre-processing + input + output of the Model at same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7f5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_parse(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    upsampled = torch.nn.functional.interpolate(\n",
    "        logits, size=image.size[::-1], mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    mask = upsampled.argmax(dim=1)[0].numpy()\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4a9d5",
   "metadata": {},
   "source": [
    "### Id's to Human readable Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f7767a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Background',\n",
       " 1: 'Hat',\n",
       " 2: 'Hair',\n",
       " 3: 'Sunglasses',\n",
       " 4: 'Upper-clothes',\n",
       " 5: 'Skirt',\n",
       " 6: 'Pants',\n",
       " 7: 'Dress',\n",
       " 8: 'Belt',\n",
       " 9: 'Left-shoe',\n",
       " 10: 'Right-shoe',\n",
       " 11: 'Face',\n",
       " 12: 'Left-leg',\n",
       " 13: 'Right-leg',\n",
       " 14: 'Left-arm',\n",
       " 15: 'Right-arm',\n",
       " 16: 'Bag',\n",
       " 17: 'Scarf'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"matei-dorian/segformer-b5-finetuned-human-parsing\")\n",
    "id2label = config.id2label\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a9a4a",
   "metadata": {},
   "source": [
    "## Now we can do something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989f8a2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Code Playground/wardrob-ai/experiments/output/masks/parsed_output.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image, mask \u001b[38;5;241m=\u001b[39m human_parse(img_path)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvisualize_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36mvisualize_mask\u001b[1;34m(mask, save_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_id, color \u001b[38;5;129;01min\u001b[39;00m ATR_COLORS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     35\u001b[0m     color_mask[mask \u001b[38;5;241m==\u001b[39m class_id] \u001b[38;5;241m=\u001b[39m color\n\u001b[1;32m---> 37\u001b[0m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput mask saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\PIL\\Image.py:2576\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2574\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2575\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2576\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2578\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Code Playground/wardrob-ai/experiments/output/masks/parsed_output.png'"
     ]
    }
   ],
   "source": [
    "image, mask = human_parse(img_path)\n",
    "visualize_mask(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac378dc",
   "metadata": {},
   "source": [
    "## Now its time to extract the required mask\n",
    "\n",
    "##### 1. How do we know which masks are required?\n",
    "- We know this because all virtual try-on research papers and official implementations follow the same preprocessing stage.\n",
    "    Here are the main sources:\n",
    "\n",
    "    - VITON (2018) paper + code\n",
    "\n",
    "    - CP-VTON (2018)\n",
    "\n",
    "    - CP-VTON+ (2020)\n",
    "\n",
    "    - VITON-HD (2022)\n",
    "\n",
    "    - HR-VITON (2022)\n",
    "\n",
    "    - TryOnDiffusion (2023)\n",
    "\n",
    "##### 2. Why the current Raw segmentation mask is not enough?\n",
    "- because it looks like this: ``` [0 2 4 6 9 10 11 ...] ``` and this is only a class label map.\n",
    "- The try-on system needs binary masks for:\n",
    "    - removing the old clothes\n",
    "    - protecting skin\n",
    "    - fitting new clothes\n",
    "    - warping garments\n",
    "    - generating the human body shape\n",
    "- You cannot feed class IDs directly into VTON pipelines.\n",
    "\n",
    "##### 3. So what masks we need?\n",
    "- mask_upper.png\n",
    "- mask_skin.png\n",
    "- mask_hair.png\n",
    "- mask_body.png\n",
    "- agnostic_person.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# ========= ATR CLASS LABELS (Your model, 18 classes) =========\n",
    "ATR_ID = {\n",
    "    \"background\": 0,\n",
    "    \"hat\": 1,\n",
    "    \"hair\": 2,\n",
    "    \"sunglasses\": 3,\n",
    "    \"upper\": 4,       # Upper-clothes\n",
    "    \"skirt\": 5,\n",
    "    \"pants\": 6,\n",
    "    \"dress\": 7,\n",
    "    \"belt\": 8,\n",
    "    \"left_shoe\": 9,\n",
    "    \"right_shoe\": 10,\n",
    "    \"face\": 11,\n",
    "    \"left_leg\": 12,\n",
    "    \"right_leg\": 13,\n",
    "    \"left_arm\": 14,\n",
    "    \"right_arm\": 15,\n",
    "    \"bag\": 16,\n",
    "    \"scarf\": 17,\n",
    "}\n",
    "\n",
    "\n",
    "# ========= MASK GROUP DEFINITIONS =========\n",
    "UPPER_CLOTHES = [ATR_ID[\"upper\"], ATR_ID[\"dress\"]]       # clothing to remove\n",
    "SKIN = [ATR_ID[\"face\"], ATR_ID[\"left_arm\"], ATR_ID[\"right_arm\"],\n",
    "        ATR_ID[\"left_leg\"], ATR_ID[\"right_leg\"]]\n",
    "HAIR = [ATR_ID[\"hair\"]]\n",
    "BACKGROUND = [ATR_ID[\"background\"]]\n",
    "\n",
    "\n",
    "def create_mask(pred_mask, class_ids):\n",
    "    \"\"\"\n",
    "    Create a binary mask for given class IDs.\n",
    "    \"\"\"\n",
    "    mask = np.isin(pred_mask, class_ids).astype(np.uint8) * 255\n",
    "    return mask\n",
    "\n",
    "\n",
    "def extract_masks(pred_mask, original_image, out_dir=\"masks\"):\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \"\"\"\n",
    "    Extract and save required WardrobeAI masks:\n",
    "        - upper clothes\n",
    "        - skin\n",
    "        - hair\n",
    "        - full body (person)\n",
    "        - agnostic person (clothes removed)\n",
    "    \"\"\"\n",
    "\n",
    "    H, W = pred_mask.shape\n",
    "\n",
    "    # Convert PIL image → numpy\n",
    "    img_np = np.array(original_image)\n",
    "\n",
    "    # MASK: UPPER CLOTHES\n",
    "    mask_upper = create_mask(pred_mask, UPPER_CLOTHES)\n",
    "    Image.fromarray(mask_upper).save(f\"{out_dir}/mask_upper.png\")\n",
    "\n",
    "    # MASK: SKIN\n",
    "    mask_skin = create_mask(pred_mask, SKIN)\n",
    "    Image.fromarray(mask_skin).save(f\"{out_dir}/mask_skin.png\")\n",
    "\n",
    "    # 3. MASK: HAIR\n",
    "    mask_hair = create_mask(pred_mask, HAIR)\n",
    "    Image.fromarray(mask_hair).save(f\"{out_dir}/mask_hair.png\")\n",
    "\n",
    "    # MASK: PERSON / BODY\n",
    "    all_body_classes = list(range(1, 18))   # everything except background\n",
    "    mask_body = create_mask(pred_mask, all_body_classes)\n",
    "    Image.fromarray(mask_body).save(f\"{out_dir}/mask_body.png\")\n",
    "\n",
    "    # AGNOSTIC PERSON\n",
    "    # Start with original\n",
    "    agnostic = img_np.copy()\n",
    "\n",
    "    # Remove upper clothes → fill with gray\n",
    "    agnostic[mask_upper == 255] = [128, 128, 128]\n",
    "\n",
    "    # Keep skin, face, hair, arms, etc. as is\n",
    "    # Background remains background\n",
    "\n",
    "    Image.fromarray(agnostic).save(f\"{out_dir}/agnostic_person.png\")\n",
    "    print(\"All masks saved in:\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f58c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All masks saved in: D:/Code Playground/wardrob-ai/experiments/masks\n"
     ]
    }
   ],
   "source": [
    "extract_masks(mask, image, out_dir = \"D:/Code Playground/wardrob-ai/experiments/masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9b55b",
   "metadata": {},
   "source": [
    "## Connecting all the Dots\n",
    "\n",
    "##### Final pipeline to automate segemantaion + mask extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eaaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmentation_pipeline(image_path):\n",
    "    image, mask = human_parse(image_path)\n",
    "    visualize_mask(mask)\n",
    "    extract_masks(mask, image, out_dir = \"D:/Code Playground/wardrob-ai/experiments/output/masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output mask saved at: parsed_output.png\n",
      "All masks saved in: D:/Code Playground/wardrob-ai/experiments/masks\n"
     ]
    }
   ],
   "source": [
    "run_segmentation_pipeline(img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
