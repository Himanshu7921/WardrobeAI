# **Segmentation Pipeline – WardrobeAI**

This module provides the complete segmentation and mask extraction pipeline for **WardrobeAI**, enabling precise human parsing, region-level masking, and the generation of agnostic images required for virtual try-on systems.

The pipeline is built on top of **SegFormer-B5**, fine-tuned for human parsing, and produces the exact masks required for downstream modules such as cloth warping and try-on generation.

---

## **Overview**

The segmentation pipeline performs:

1. **Human Parsing**
   Generates a pixel-wise class mask of the person image using a pretrained SegFormer model.

2. **Visualization**
   Converts the raw class mask into a colored segmentation map using the ATR color palette.

3. **Mask Extraction**
   Extracts task-specific binary masks required for WardrobeAI:

   * Upper-body clothing mask
   * Skin mask
   * Hair mask
   * Full-body mask
   * Agnostic person image (clothes removed)

4. **Directory-safe Saving**
   All outputs are automatically written to structured output directories.

---

## **Model Used**

**Human Parsing Model:**
SegFormer-B5 (fine-tuned on ATR human parsing dataset)

Model link (HuggingFace):
`matei-dorian/segformer-b5-finetuned-human-parsing`

This model predicts **18 body/clothing classes**, including hair, face, arms, legs, shoes, dress, skirt, etc.

---

## **Key Components**

### **1. Human Parsing**

```python
def human_parse(image_path):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    outputs = model(**inputs)
    logits = outputs.logits
    upsampled = torch.nn.functional.interpolate(
        logits, size=image.size[::-1], mode="bilinear", align_corners=False
    )
    mask = upsampled.argmax(dim=1)[0].numpy()
    return image, mask
```

This function returns:

* `image` → original PIL image
* `mask` → `H×W` class map

---

### **2. Mask Visualization**

```python
visualize_mask(mask, save_path=".../parsed_output.png")
```

This produces a color-coded segmentation map using the predefined **ATR_COLORS** palette.

---

### **3. Mask Extraction**

The pipeline extracts critical region masks needed for try-on preprocessing:

* **mask_upper.png**
* **mask_skin.png**
* **mask_hair.png**
* **mask_body.png**
* **agnostic_person.png**

```python
extract_masks(pred_mask, image, out_dir=".../masks")
```

The agnostic person is generated by removing upper clothing and replacing it with neutral gray.

---

## **ATR Class Definitions**

The model predicts 18 semantic classes:

```python
{
  0: Background,
  1: Hat,
  2: Hair,
  3: Sunglasses,
  4: Upper-clothes,
  5: Skirt,
  6: Pants,
  7: Dress,
  8: Belt,
  9: Left-shoe,
  10: Right-shoe,
  11: Face,
  12: Left-leg,
  13: Right-leg,
  14: Left-arm,
  15: Right-arm,
  16: Bag,
  17: Scarf
}
```

These are grouped into segmentation masks required for try-on preprocessing.

---

## **Folder Structure**

The pipeline outputs into:

```
data/
   segmentation_output/
       masks/
           mask_upper.png
           mask_skin.png
           mask_hair.png
           mask_body.png
           agnostic_person.png
       parsed_output.png
```

Folders are auto-created when missing.

---

## **How to Run**

Modify the input image path and run:

```bash
python backend/segmentation/pipeline.py
```

The script will:

1. Parse the human image
2. Generate the colored segmentation map
3. Extract all required masks
4. Save everything inside `data/segmentation_output/`

---

## **Usage in WardrobeAI**

This segmentation pipeline is Step 1 of the full virtual try-on pipeline.

Output masks feed into:

* Pose estimation
* Agnostic person creation
* Cloth warping module
* Try-on generator (HR-VITON / TryOnDiffusion)

---

## **Dependencies**

This module requires:

* Python 3.10
* PyTorch (CUDA 12.1 build recommended)
* Transformers
* Pillow
* NumPy
* Pandas

All pinned versions are listed in your `requirements.txt`.

---

## **Notes**

* Input images must contain a single person in a standing pose for best results.
* This pipeline is optimized for clean front-facing/full-body images.
* All generated outputs are excluded via `.gitignore` for privacy and repository cleanliness.
